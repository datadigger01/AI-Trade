{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "-TZLFWfACNe6",
        "outputId": "38223c5a-a3bf-4f89-c1ab-c95fbe052ae1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bertopic\n",
            "  Downloading bertopic-0.17.3-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: hdbscan>=0.8.29 in /usr/local/lib/python3.12/dist-packages (from bertopic) (0.8.40)\n",
            "Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from bertopic) (0.5.9.post2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.12/dist-packages (from bertopic) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.12/dist-packages (from bertopic) (2.2.2)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from bertopic) (5.24.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.12/dist-packages (from bertopic) (1.6.1)\n",
            "Requirement already satisfied: sentence-transformers>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from bertopic) (5.1.1)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.12/dist-packages (from bertopic) (4.67.1)\n",
            "Requirement already satisfied: llvmlite>0.36.0 in /usr/local/lib/python3.12/dist-packages (from bertopic) (0.43.0)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.12/dist-packages (from hdbscan>=0.8.29->bertopic) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from hdbscan>=0.8.29->bertopic) (1.5.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly>=4.7.0->bertopic) (8.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from plotly>=4.7.0->bertopic) (25.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0->bertopic) (3.6.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.57.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=0.4.1->bertopic) (2.8.0+cu126)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.35.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=0.4.1->bertopic) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.15.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.12/dist-packages (from umap-learn>=0.5.0->bertopic) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.12/dist-packages (from umap-learn>=0.5.0->bertopic) (0.5.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (1.1.10)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.17.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.6.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2025.10.5)\n",
            "Downloading bertopic-0.17.3-py3-none-any.whl (153 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.0/153.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bertopic\n",
            "Successfully installed bertopic-0.17.3\n",
            "Requirement already satisfied: bertopic[flair,gensim,spacy,use,visualization] in /usr/local/lib/python3.12/dist-packages (0.17.3)\n",
            "\u001b[33mWARNING: bertopic 0.17.3 does not provide the extra 'visualization'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: hdbscan>=0.8.29 in /usr/local/lib/python3.12/dist-packages (from bertopic[flair,gensim,spacy,use,visualization]) (0.8.40)\n",
            "Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from bertopic[flair,gensim,spacy,use,visualization]) (0.5.9.post2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.12/dist-packages (from bertopic[flair,gensim,spacy,use,visualization]) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.12/dist-packages (from bertopic[flair,gensim,spacy,use,visualization]) (2.2.2)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from bertopic[flair,gensim,spacy,use,visualization]) (5.24.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.12/dist-packages (from bertopic[flair,gensim,spacy,use,visualization]) (1.6.1)\n",
            "Requirement already satisfied: sentence-transformers>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from bertopic[flair,gensim,spacy,use,visualization]) (5.1.1)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.12/dist-packages (from bertopic[flair,gensim,spacy,use,visualization]) (4.67.1)\n",
            "Requirement already satisfied: llvmlite>0.36.0 in /usr/local/lib/python3.12/dist-packages (from bertopic[flair,gensim,spacy,use,visualization]) (0.43.0)\n",
            "Collecting flair>=0.7 (from bertopic[flair,gensim,spacy,use,visualization])\n",
            "  Downloading flair-0.15.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from bertopic[flair,gensim,spacy,use,visualization]) (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers>=3.5.1 in /usr/local/lib/python3.12/dist-packages (from bertopic[flair,gensim,spacy,use,visualization]) (4.57.0)\n",
            "Collecting gensim>=4.0.0 (from bertopic[flair,gensim,spacy,use,visualization])\n",
            "  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: spacy>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from bertopic[flair,gensim,spacy,use,visualization]) (3.8.7)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (from bertopic[flair,gensim,spacy,use,visualization]) (2.19.0)\n",
            "Requirement already satisfied: tensorflow_hub in /usr/local/lib/python3.12/dist-packages (from bertopic[flair,gensim,spacy,use,visualization]) (0.16.1)\n",
            "Requirement already satisfied: tensorflow_text in /usr/local/lib/python3.12/dist-packages (from bertopic[flair,gensim,spacy,use,visualization]) (2.19.0)\n",
            "Collecting boto3>=1.20.27 (from flair>=0.7->bertopic[flair,gensim,spacy,use,visualization])\n",
            "  Downloading boto3-1.40.54-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting conllu<5.0.0,>=4.0 (from flair>=0.7->bertopic[flair,gensim,spacy,use,visualization])\n",
            "  Downloading conllu-4.5.3-py2.py3-none-any.whl.metadata (19 kB)\n",
            "Collecting deprecated>=1.2.13 (from flair>=0.7->bertopic[flair,gensim,spacy,use,visualization])\n",
            "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting ftfy>=6.1.0 (from flair>=0.7->bertopic[flair,gensim,spacy,use,visualization])\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: gdown>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from flair>=0.7->bertopic[flair,gensim,spacy,use,visualization]) (5.2.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from flair>=0.7->bertopic[flair,gensim,spacy,use,visualization]) (0.35.3)\n",
            "Collecting langdetect>=1.0.9 (from flair>=0.7->bertopic[flair,gensim,spacy,use,visualization])\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from flair>=0.7->bertopic[flair,gensim,spacy,use,visualization]) (5.4.0)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.12/dist-packages (from flair>=0.7->bertopic[flair,gensim,spacy,use,visualization]) (3.10.0)\n",
            "Requirement already satisfied: more-itertools>=8.13.0 in /usr/local/lib/python3.12/dist-packages (from flair>=0.7->bertopic[flair,gensim,spacy,use,visualization]) (10.8.0)\n",
            "Collecting mpld3>=0.3 (from flair>=0.7->bertopic[flair,gensim,spacy,use,visualization])\n",
            "  Downloading mpld3-0.5.11-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting pptree>=3.1 (from flair>=0.7->bertopic[flair,gensim,spacy,use,visualization])\n",
            "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from flair>=0.7->bertopic[flair,gensim,spacy,use,visualization]) (2.9.0.post0)\n",
            "Collecting pytorch-revgrad>=0.2.0 (from flair>=0.7->bertopic[flair,gensim,spacy,use,visualization])\n",
            "  Downloading pytorch_revgrad-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from flair>=0.7->bertopic[flair,gensim,spacy,use,visualization]) (2024.11.6)\n",
            "Collecting segtok>=1.5.11 (from flair>=0.7->bertopic[flair,gensim,spacy,use,visualization])\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting sqlitedict>=2.0.0 (from flair>=0.7->bertopic[flair,gensim,spacy,use,visualization])\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.12/dist-packages (from flair>=0.7->bertopic[flair,gensim,spacy,use,visualization]) (0.9.0)\n",
            "Collecting transformer-smaller-training-vocab>=0.2.3 (from flair>=0.7->bertopic[flair,gensim,spacy,use,visualization])\n",
            "  Downloading transformer_smaller_training_vocab-0.4.2-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting wikipedia-api>=0.5.7 (from flair>=0.7->bertopic[flair,gensim,spacy,use,visualization])\n",
            "  Downloading wikipedia_api-0.8.1.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bioc<3.0.0,>=2.0.0 (from flair>=0.7->bertopic[flair,gensim,spacy,use,visualization])\n",
            "  Downloading bioc-2.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting numpy>=1.20.0 (from bertopic[flair,gensim,spacy,use,visualization])\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim>=4.0.0->bertopic[flair,gensim,spacy,use,visualization])\n",
            "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim>=4.0.0->bertopic[flair,gensim,spacy,use,visualization]) (7.3.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from hdbscan>=0.8.29->bertopic[flair,gensim,spacy,use,visualization]) (1.5.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->bertopic[flair,gensim,spacy,use,visualization]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->bertopic[flair,gensim,spacy,use,visualization]) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly>=4.7.0->bertopic[flair,gensim,spacy,use,visualization]) (8.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from plotly>=4.7.0->bertopic[flair,gensim,spacy,use,visualization]) (25.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0->bertopic[flair,gensim,spacy,use,visualization]) (3.6.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=0.4.1->bertopic[flair,gensim,spacy,use,visualization]) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=0.4.1->bertopic[flair,gensim,spacy,use,visualization]) (4.15.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.0.1->bertopic[flair,gensim,spacy,use,visualization]) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.0.1->bertopic[flair,gensim,spacy,use,visualization]) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.0.1->bertopic[flair,gensim,spacy,use,visualization]) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.0.1->bertopic[flair,gensim,spacy,use,visualization]) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.0.1->bertopic[flair,gensim,spacy,use,visualization]) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.0.1->bertopic[flair,gensim,spacy,use,visualization]) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.0.1->bertopic[flair,gensim,spacy,use,visualization]) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.0.1->bertopic[flair,gensim,spacy,use,visualization]) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.0.1->bertopic[flair,gensim,spacy,use,visualization]) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.0.1->bertopic[flair,gensim,spacy,use,visualization]) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.0.1->bertopic[flair,gensim,spacy,use,visualization]) (0.19.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.0.1->bertopic[flair,gensim,spacy,use,visualization]) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.0.1->bertopic[flair,gensim,spacy,use,visualization]) (2.11.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.0.1->bertopic[flair,gensim,spacy,use,visualization]) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy>=3.0.1->bertopic[flair,gensim,spacy,use,visualization]) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.0.1->bertopic[flair,gensim,spacy,use,visualization]) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bertopic[flair,gensim,spacy,use,visualization]) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bertopic[flair,gensim,spacy,use,visualization]) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bertopic[flair,gensim,spacy,use,visualization]) (3.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bertopic[flair,gensim,spacy,use,visualization]) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bertopic[flair,gensim,spacy,use,visualization]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bertopic[flair,gensim,spacy,use,visualization]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bertopic[flair,gensim,spacy,use,visualization]) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bertopic[flair,gensim,spacy,use,visualization]) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bertopic[flair,gensim,spacy,use,visualization]) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bertopic[flair,gensim,spacy,use,visualization]) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bertopic[flair,gensim,spacy,use,visualization]) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bertopic[flair,gensim,spacy,use,visualization]) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bertopic[flair,gensim,spacy,use,visualization]) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bertopic[flair,gensim,spacy,use,visualization]) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bertopic[flair,gensim,spacy,use,visualization]) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bertopic[flair,gensim,spacy,use,visualization]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bertopic[flair,gensim,spacy,use,visualization]) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bertopic[flair,gensim,spacy,use,visualization]) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bertopic[flair,gensim,spacy,use,visualization]) (3.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.5.1->bertopic[flair,gensim,spacy,use,visualization]) (6.0.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.5.1->bertopic[flair,gensim,spacy,use,visualization]) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.5.1->bertopic[flair,gensim,spacy,use,visualization]) (0.6.2)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.12/dist-packages (from umap-learn>=0.5.0->bertopic[flair,gensim,spacy,use,visualization]) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.12/dist-packages (from umap-learn>=0.5.0->bertopic[flair,gensim,spacy,use,visualization]) (0.5.13)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow->bertopic[flair,gensim,spacy,use,visualization]) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow->bertopic[flair,gensim,spacy,use,visualization]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow->bertopic[flair,gensim,spacy,use,visualization]) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow->bertopic[flair,gensim,spacy,use,visualization]) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow->bertopic[flair,gensim,spacy,use,visualization]) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow->bertopic[flair,gensim,spacy,use,visualization]) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow->bertopic[flair,gensim,spacy,use,visualization]) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow->bertopic[flair,gensim,spacy,use,visualization]) (5.29.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow->bertopic[flair,gensim,spacy,use,visualization]) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow->bertopic[flair,gensim,spacy,use,visualization]) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow->bertopic[flair,gensim,spacy,use,visualization]) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow->bertopic[flair,gensim,spacy,use,visualization]) (1.75.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow->bertopic[flair,gensim,spacy,use,visualization]) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow->bertopic[flair,gensim,spacy,use,visualization]) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow->bertopic[flair,gensim,spacy,use,visualization]) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow->bertopic[flair,gensim,spacy,use,visualization]) (0.5.3)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow_hub->bertopic[flair,gensim,spacy,use,visualization]) (2.19.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow->bertopic[flair,gensim,spacy,use,visualization]) (0.45.1)\n",
            "Collecting jsonlines>=1.2.0 (from bioc<3.0.0,>=2.0.0->flair>=0.7->bertopic[flair,gensim,spacy,use,visualization])\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting intervaltree (from bioc<3.0.0,>=2.0.0->flair>=0.7->bertopic[flair,gensim,spacy,use,visualization])\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting docopt (from bioc<3.0.0,>=2.0.0->flair>=0.7->bertopic[flair,gensim,spacy,use,visualization])\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting botocore<1.41.0,>=1.40.54 (from boto3>=1.20.27->flair>=0.7->bertopic[flair,gensim,spacy,use,visualization])\n",
            "  Downloading botocore-1.40.54-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.20.27->flair>=0.7->bertopic[flair,gensim,spacy,use,visualization])\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.15.0,>=0.14.0 (from boto3>=1.20.27->flair>=0.7->bertopic[flair,gensim,spacy,use,visualization])\n",
            "  Downloading s3transfer-0.14.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy>=6.1.0->flair>=0.7->bertopic[flair,gensim,spacy,use,visualization]) (0.2.14)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown>=4.4.0->flair>=0.7->bertopic[flair,gensim,spacy,use,visualization]) (4.13.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.10.0->flair>=0.7->bertopic[flair,gensim,spacy,use,visualization]) (1.1.10)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow->bertopic[flair,gensim,spacy,use,visualization]) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow->bertopic[flair,gensim,spacy,use,visualization]) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow->bertopic[flair,gensim,spacy,use,visualization]) (0.17.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3.0.1->bertopic[flair,gensim,spacy,use,visualization]) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.2.3->flair>=0.7->bertopic[flair,gensim,spacy,use,visualization]) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.2.3->flair>=0.7->bertopic[flair,gensim,spacy,use,visualization]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.2.3->flair>=0.7->bertopic[flair,gensim,spacy,use,visualization]) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.2.3->flair>=0.7->bertopic[flair,gensim,spacy,use,visualization]) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.2.3->flair>=0.7->bertopic[flair,gensim,spacy,use,visualization]) (3.2.5)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.1->bertopic[flair,gensim,spacy,use,visualization]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.1->bertopic[flair,gensim,spacy,use,visualization]) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.1->bertopic[flair,gensim,spacy,use,visualization]) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.1->bertopic[flair,gensim,spacy,use,visualization]) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.1->bertopic[flair,gensim,spacy,use,visualization]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.1->bertopic[flair,gensim,spacy,use,visualization]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.1->bertopic[flair,gensim,spacy,use,visualization]) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.4.0->bertopic[flair,gensim,spacy,use,visualization]) (1.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow->bertopic[flair,gensim,spacy,use,visualization]) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow->bertopic[flair,gensim,spacy,use,visualization]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow->bertopic[flair,gensim,spacy,use,visualization]) (3.1.3)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy>=3.0.1->bertopic[flair,gensim,spacy,use,visualization]) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy>=3.0.1->bertopic[flair,gensim,spacy,use,visualization]) (0.1.5)\n",
            "INFO: pip is looking at multiple versions of thinc to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting thinc<8.4.0,>=8.3.4 (from spacy>=3.0.1->bertopic[flair,gensim,spacy,use,visualization])\n",
            "  Downloading thinc-8.3.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy>=3.0.1->bertopic[flair,gensim,spacy,use,visualization])\n",
            "  Downloading blis-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair>=0.7->bertopic[flair,gensim,spacy,use,visualization]) (0.2.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0.1->bertopic[flair,gensim,spacy,use,visualization]) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0.1->bertopic[flair,gensim,spacy,use,visualization]) (1.5.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.0.1->bertopic[flair,gensim,spacy,use,visualization]) (0.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy>=3.0.1->bertopic[flair,gensim,spacy,use,visualization]) (3.0.3)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonlines>=1.2.0->bioc<3.0.0,>=2.0.0->flair>=0.7->bertopic[flair,gensim,spacy,use,visualization]) (25.4.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.0.1->bertopic[flair,gensim,spacy,use,visualization]) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow->bertopic[flair,gensim,spacy,use,visualization]) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow->bertopic[flair,gensim,spacy,use,visualization]) (2.19.2)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair>=0.7->bertopic[flair,gensim,spacy,use,visualization]) (1.10.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown>=4.4.0->flair>=0.7->bertopic[flair,gensim,spacy,use,visualization]) (2.8)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from intervaltree->bioc<3.0.0,>=2.0.0->flair>=0.7->bertopic[flair,gensim,spacy,use,visualization]) (2.4.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown>=4.4.0->flair>=0.7->bertopic[flair,gensim,spacy,use,visualization]) (1.7.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.26.0->transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair>=0.7->bertopic[flair,gensim,spacy,use,visualization]) (5.9.5)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow->bertopic[flair,gensim,spacy,use,visualization]) (0.1.2)\n",
            "Downloading flair-0.15.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bioc-2.1-py3-none-any.whl (33 kB)\n",
            "Downloading boto3-1.40.54-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
            "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mpld3-0.5.11-py3-none-any.whl (202 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.9/202.9 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n",
            "Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Downloading thinc-8.3.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformer_smaller_training_vocab-0.4.2-py3-none-any.whl (14 kB)\n",
            "Downloading blis-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.40.54-py3-none-any.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading s3transfer-0.14.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: langdetect, pptree, sqlitedict, wikipedia-api, docopt, intervaltree\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=f8eb0d232df7da07065d10eb5124568bd41b24e6d9d0d7b7f7a465d5914b72b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/67/88/e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
            "  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4609 sha256=77845eddb27b5fa6bc757c789ddc6d2ddde33a6bec33214d8f96c39a4e366269\n",
            "  Stored in directory: /root/.cache/pip/wheels/b0/2d/de/37058114a8f07cfec75747cb46b864bc5c71b0e9e0e4cd0acd\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16862 sha256=03c191d9130199963f057ef4063d82566fa2b6e36f014cc4fb15b86ef0fb88b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/6f/21/fc016aef45ffcabe27129a2252f061387cbf278d2086225a64\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.8.1-py3-none-any.whl size=15383 sha256=67b7784b1c128806f8ce6e579e1ea65ca9b2c47504a965d873f102fb72619285\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/3c/79/b36253689d838af4a0539782853ac3cc38a83a6591ad570dde\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=404f91af650d5a44596b3c6003ed9cf732635095066be17988f44a03e3ec6a7b\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/bf/a1/4cee4f7678c68c5875ca89eaccf460593539805c3906722228\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26098 sha256=be9f63452af070d32193306c82a37f6a362ecfe5e7f48c8b030f457274154b5a\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/c3/c3/238bf93c243597857edd94ddb0577faa74a8e16e9585896e83\n",
            "Successfully built langdetect pptree sqlitedict wikipedia-api docopt intervaltree\n",
            "Installing collected packages: sqlitedict, pptree, docopt, segtok, numpy, langdetect, jsonlines, jmespath, intervaltree, ftfy, deprecated, conllu, wikipedia-api, scipy, botocore, blis, bioc, s3transfer, gensim, thinc, pytorch-revgrad, mpld3, boto3, transformer-smaller-training-vocab, flair\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.2\n",
            "    Uninstalling scipy-1.16.2:\n",
            "      Successfully uninstalled scipy-1.16.2\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 1.3.0\n",
            "    Uninstalling blis-1.3.0:\n",
            "      Successfully uninstalled blis-1.3.0\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.3.6\n",
            "    Uninstalling thinc-8.3.6:\n",
            "      Successfully uninstalled thinc-8.3.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bioc-2.1 blis-1.2.1 boto3-1.40.54 botocore-1.40.54 conllu-4.5.3 deprecated-1.2.18 docopt-0.6.2 flair-0.15.1 ftfy-6.3.1 gensim-4.3.3 intervaltree-3.1.0 jmespath-1.0.1 jsonlines-4.0.0 langdetect-1.0.9 mpld3-0.5.11 numpy-1.26.4 pptree-3.1 pytorch-revgrad-0.2.0 s3transfer-0.14.0 scipy-1.13.1 segtok-1.5.11 sqlitedict-2.1.0 thinc-8.3.4 transformer-smaller-training-vocab-0.4.2 wikipedia-api-0.8.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "4fda8e6cfd424fb6a1d3006935d28ae6"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 1. Package Installation\n",
        "# ==========================================\n",
        "\n",
        "!pip install bertopic\n",
        "#!pip install --upgrade gensim\n",
        "\n",
        "# Choose an embedding backend\n",
        "!pip install bertopic[flair,gensim,spacy,use,visualization]\n",
        "\n",
        "# Topic modeling with images\n",
        "# !pip install bertopic[vision]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Qt2t4zMMYlTf",
        "outputId": "ff2b1d81-62e4-405a-d101-d261e02a442e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading jpype1-1.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from konlpy) (5.4.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.12/dist-packages (from konlpy) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from JPype1>=0.7.0->konlpy) (25.0)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jpype1-1.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (495 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m495.9/495.9 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.6.0 konlpy-0.6.0\n",
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.4.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from pyLDAvis) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from pyLDAvis) (1.13.1)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pyLDAvis) (2.2.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from pyLDAvis) (1.5.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from pyLDAvis) (3.1.6)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.12/dist-packages (from pyLDAvis) (2.13.1)\n",
            "Collecting funcy (from pyLDAvis)\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from pyLDAvis) (1.6.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (from pyLDAvis) (4.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from pyLDAvis) (75.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->pyLDAvis) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->pyLDAvis) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim->pyLDAvis) (7.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->pyLDAvis) (3.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim->pyLDAvis) (1.17.3)\n",
            "Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Installing collected packages: funcy, pyLDAvis\n",
            "Successfully installed funcy-2.0 pyLDAvis-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install konlpy\n",
        "!pip install pyLDAvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fi6ZrZugCUGm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "708d7d18-bac4-4bd1-866a-e192f9a7ecd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/hdbscan/robust_single_linkage_.py:175: SyntaxWarning: invalid escape sequence '\\{'\n",
            "  $max \\{ core_k(a), core_k(b), 1/\\alpha d(a,b) \\}$.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "한국어 텍스트 데이터를 위한 BERTopic 모델링\n",
        "파일: trade_success_case_data.csv의 bdtCntnt 컬럼 분석\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from konlpy.tag import Okt, Hannanum, Kkma, Komoran\n",
        "#from konlpy.tag import Okt\n",
        "\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "\n",
        "import string\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amNlIefHEdaw"
      },
      "source": [
        "#### ============================================\n",
        "#### 1. 데이터 로드\n",
        "#### ============================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDqJT3vtDNe-",
        "outputId": "03937070-0d00-40d1-885e-903f6d463167"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "1. 데이터 로드 중...\n",
            "============================================================\n",
            "전체 데이터 수: 194\n",
            "유효한 텍스트 수: 194\n",
            "평균 텍스트 길이: 2746자\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 194 entries, 0 to 193\n",
            "Data columns (total 10 columns):\n",
            " #   Column     Non-Null Count  Dtype         \n",
            "---  ------     --------------  -----         \n",
            " 0   bdtCntnt   194 non-null    object        \n",
            " 1   dataType   194 non-null    object        \n",
            " 2   ovrofInfo  0 non-null      float64       \n",
            " 3   dmgeAmt    0 non-null      float64       \n",
            " 4   fraudType  0 non-null      float64       \n",
            " 5   othbcDt    194 non-null    datetime64[ns]\n",
            " 6   bbstxSn    194 non-null    int64         \n",
            " 7   titl       194 non-null    object        \n",
            " 8   natn       194 non-null    object        \n",
            " 9   regn       194 non-null    object        \n",
            "dtypes: datetime64[ns](1), float64(3), int64(1), object(5)\n",
            "memory usage: 15.3+ KB\n",
            "데이터셋 정보:None\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# 1. 데이터 로드\n",
        "# ============================================\n",
        "print(\"=\" * 60)\n",
        "print(\"1. 데이터 로드 중...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/datadigger01/AI-Trade/main/Data/trade_success_case_data.csv')\n",
        "df['othbcDt'] = pd.to_datetime(df['othbcDt'])\n",
        "\n",
        "print(f\"전체 데이터 수: {len(df)}\")\n",
        "\n",
        "# bdtCntnt 컬럼 추출 및 결측치 제거\n",
        "texts = df['bdtCntnt'].dropna().tolist()\n",
        "timestamps = df['othbcDt'].dt.year.to_list() # 토픽별 Time Series를 위한 년단위 timestamps\n",
        "\n",
        "print(f\"유효한 텍스트 수: {len(texts)}\")\n",
        "print(f\"평균 텍스트 길이: {np.mean([len(text) for text in texts]):.0f}자\\n\")\n",
        "print(f\"데이터셋 정보:{df.info()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "tKxXlMCYDSjs",
        "outputId": "e69d5f1a-d6d6-4880-9017-59b1c85e93c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'국내 최초 UV-C LED 칫솔 살균기 개발2000년 설립된 비에네스소프트는 소프트웨어 개발 기업이자 \\'울트라웨이브\\'라는 브랜드로살균에 초점을 맞춘 퍼스널 헬스케어 제품을 개발&middot;제조&middot;수출하는 기업이다.독립채산제로 운영되는 특성상 회사 내 아이담 사업부에서 \\'울트라웨이브\\' 사업을 총괄한다.원래 아이담 사업부에서는 보이스레코더와 MP3 같은 음향기기를 대부분 ODM 방식으로납품해 왔다. 스마트폰 사용이 보편화되면서 음향기기 시장 규모가 줄어들자 친환경 반도체 광원인UV LED로 눈을 돌려 2018년 국내 최초로 UV-C LED 칫솔 살균기 개발에 성공했다.기존 UV램프 칫솔 살균기는 UV램프 안에 수은이 들어 있고, 살균 과정에서 오존을 발생기키기때문에 비릿한 냄새가 나는 단점이 있었다. UV LED 파장 중 UV-C를 사용한 울트라웨이브칫솔 살균기는 3분이면 뮤탄스균&middot;녹농균&middot;대장균&middot;황색포도상구균을 99.9%를 살균한다.수은을 함유하고 있지 않아 친환경적이고, 오존을 발생시키지 않아 불쾌감 없이 쓸 수 있다.EU의 전기&middot;전자제품 유해물질사용제한(RoHS) 인증을 취득했고,미국 FDA 승인, 미국 FOC 인증, 유럽 CD 인증도 완료했다.화상상담으로 해외 시장 확대 가능성을 보다비에네스소프트는 칫솔 살균기, 면도기 살균기, 마스크 살균기 등 UV LED 살균기 라인과 함께EARPET(반려동물의 귀 질환&middot;염증을 치료하는 동물의료기기,PEPPI3(반려동물 전용 MP3) 등펫헬스케어 라인을 구성했지만 2020년, 코로나19라는 변수를 맞았다.코로나19로 참가 예정이었던 해외 전시회가 줄줄이 취소되는 상황이 발생한 것이다.   \"2019년에 해외 시장 조사와 신규 바이어 영업을 위해 KOTRA에 도움을 요청했다면, 2020년에는 영국, 스페인, 독일, 미국, 싱가포르, 홍콩, 말레이시아, 태국, 칠레 등 여러 국가의 기업들과 화상상담회를 진행했습니다. 해외 시장을 확대하려는 중소기업, 특히 신제품을 개발한 기업에게는 해외 전시회 참여가 필수인데, 코로나10로 인해 해외 출장마저 갈 수 없는 막막한 상황에서 화상상담회는 희망이 되었습니다.\"   특히 2020년 6월 진행한 함부르크 무역관 화상상담회는 무엇보다 그 성과가 두드러진화상상담회였다. 비에네스소프트는 6월 초, 함부르크 무역관으로부터 제품을 유통하고자 하는S사와의 화상상담을 제안 받아 6월 15일 화상상담을 진행했다. S사는 쿠쿠전자 유럽총판 지원업체로, 아마존 독일, OTTO 등 유럽 내 온라인 플랫폼과 한국 롯데 하이마트와유사한 Saturn, MediaMark 등 오프라인 유통라인을 보유하고 있다.화상상담회 덕분에 독일의 S사에 샘플 물량을 추출할 수 있었다.세계 퍼스널 헬스케어 살균기 시장 1위를 꿈꾸다     \"S사에서 울트라웨이브 제품에 대해 큰 관심과 기대감을 표현해 주셨습니다. 제품 기능 위주로 질문과 답변이 이뤄졌고, 양사가 추구하는 유통 방향에 대해서도 협의할 수 있었습니다.\"   S사에 테스트 물량을 수출한 비에네스소프트는 현재 아마존 독일 론칭을 준비 중이다.또한 S사와 독일뿐 아니라 EU 전체로도 제품 유통이 가능한 지에 대해서도 논의하고 있다.비에네스소프트는 여러 차례의 화상상담회를 계기로 2020년 하반기부터 2021년까지200~300만 달러 규모의 신규 수출을 예상하고 있다.코로나19는 오히려 비에네스소프트에 새로운 기회를 가져다주었다. 코로나19 이전에는바이어에게 퍼스널 헬스케어 살균기가 왜 필요한지 오랜 시간을 들여 설득해야 했지만,코로나19 대유행 이후부터는 그럴 필요가 없어졌다. 2018년 세계 최초로 개발해 2019년국내 특허 등록까지 마친 마스크 살균기에 대한 관심도 폭발적으로 증가했다.살균기는 기본적으로 신뢰성을 담보해야 한다. 비에네스소프트는 국내&middot;해외 인증 결과로고객에게 제품에 대한 신뢰를 제공하고 있다. 여기에 한국의 성공적인 코로나19 방역을전 세계가 주목하면서 한국 제품으로서의 가치가 더 상승했다. 게다가 KOTRA와 함께해외 업체들과 접촉하기 때문에 비에네스소프트에 대한 신뢰를 더욱 높일 수 있었다.비에네스소프트 입장에서는 해외 시장에 대한 폭넓고 정확한 정보를 가진KOTRA를 통해 연결되는 기업이라면 신뢰할 수 있다는 믿음도 생겼다.사람과 반려동물을 대상으로 광(光) 테라피를 이용한 헬스케어 제품을 보유한 비에네스소프트는앞으로 공기 살균기처럼 공간을 바꾸고 생활문화까지 바꿀 수 있는 제품을 선보일 계획인다.또한 현재 매출의 70%가 해외 시장에서 발생하고 있어 미국, 남미를 중심으로해외 시장 개척&middot;확대에 더욱 박차를 가할 예정이다.     \"KOTRA와 지속적인 협력을 통해 미주, 유럽, 아세안, 서아시아 등으로 시장을 확대해 한국 브랜드인 울트라웨이브가 글로벌 퍼스널 헬스케어 살균기 시장 1위로 우뚝 설 수 있도록 노력하겠습니다.\"   #출처: 경제외교, 해외진출 길을 넓히다: 경제외교 기업활용 성과사례집 (바로가기)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "texts[11]\n",
        "#timestamps[3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShfQWIvCEnFn"
      },
      "source": [
        "#### ============================================\n",
        "#### 2. 텍스트 전처리\n",
        "#### ============================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3fEZKEIDdAf",
        "outputId": "9608aa89-8801-4fd8-9a3d-d2c59721891f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "2. 텍스트 전처리 중...\n",
            "============================================================\n",
            "전처리 완료!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# 2. 텍스트 전처리\n",
        "# ============================================\n",
        "print(\"=\" * 60)\n",
        "print(\"2. 텍스트 전처리 중...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def combine_specific_words(text, word_pairs):\n",
        "    \"\"\"특정 단어 조합을 하나로 합치는 함수\"\"\"\n",
        "    for word_pair in word_pairs:\n",
        "        # 공백이 있는 형태를 공백 없는 형태로 변환\n",
        "        text = text.replace(word_pair, word_pair.replace(' ', ''))\n",
        "    return text\n",
        "\n",
        "def replace_specific_words(text, word_dict):\n",
        "    \"\"\"특정 단어를 다른 단어로 치환하는 함수\"\"\"\n",
        "    for old_word, new_word in word_dict.items():\n",
        "        text = text.replace(old_word, new_word)\n",
        "    return text\n",
        "\n",
        "def preprocess_korean_text(text):\n",
        "    \"\"\"한국어 텍스트 전처리\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"  ## doucument가 결측인경우 NaN return\n",
        "\n",
        "    #########  특수문자 포함 정제처리할 단어 처리(삭제)\n",
        "    # HTML 태그 제거\n",
        "    pattern = r'<[^>]*>'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    ## HTML 태그 제거\n",
        "    ##text = re.sub(r'&[a-z]+;', ' ', text)\n",
        "\n",
        "    # E-mail제거\n",
        "    pattern = r'([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)'\n",
        "    text = re.sub(pattern, '', text)\n",
        "\n",
        "    # URL제거\n",
        "    pattern = r'(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    ## Remove URLs\n",
        "    # text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
        "\n",
        "    # 특수문자 제거 (한글, 영문, 공백만 유지)\n",
        "    text = re.sub(r'[^가-힣a-zA-Z\\s]', ' ', text)\n",
        "    #text = re.sub(r'[^가-힣a-zA-Z0-9\\s]', ' ', text)\n",
        "\n",
        "    # 한글 자음, 모음 제거\n",
        "    pattern = r'([ㄱ-ㅎㅏ-ㅣ]+)'\n",
        "    text = re.sub(pattern, '', text)\n",
        "\n",
        "    # 연속된 공백 제거\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # # 연속 공백 정리\n",
        "    # text = ' '.join(text.split())\n",
        "\n",
        "    # 특수기호제거\n",
        "    #pattern = r'[^\\w\\s]'\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Remove mentions and hashtags\n",
        "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # 영어는 모두 대문자로 전환\n",
        "    text = text.upper()\n",
        "\n",
        "    ###########################################################################\n",
        "    ##  특정 단어에 대한 변환, 조합 및 제거\n",
        "    ###########################################################################\n",
        "    # 토큰화 전에 특정 단어를 다른 단어로 치환\n",
        "    word_replacements = {\n",
        "        'KOTRA': '코트라',\n",
        "        '어플리케이션': '애플리케이션',\n",
        "        '카메라': '캠코더',  # 예시\n",
        "        # 필요한 단어 치환을 여기에 추가하세요\n",
        "    }\n",
        "    text = replace_specific_words(text, word_replacements)\n",
        "\n",
        "    # 토큰화 전에 특정 단어 조합을 먼저 결합\n",
        "    word_pairs_to_combine = [\n",
        "        '성공 사례',\n",
        "        '중소 기업',\n",
        "        '수출 전략',\n",
        "        '시장 조사',\n",
        "        '데이터 분석',\n",
        "        '제품 개발',\n",
        "        '생활 용품'\n",
        "        # 필요한 단어 조합을 여기에 추가하세요\n",
        "    ]\n",
        "    text = combine_specific_words(text, word_pairs_to_combine)\n",
        "\n",
        "    # 특정 단어 제거\n",
        "    remove_words = ['바로가기',\n",
        "                    '성공사례',\n",
        "                    'RSQUO','LSQUO','LDQUO','RDQUO',\n",
        "                    '원문 슬기로운 코트라 활용법',\n",
        "                    '수출전문위원이 전하는 중소기업 수출성공스토리'\n",
        "                    ]\n",
        "    for word in remove_words:\n",
        "        text = text.replace(word, '')\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "# 전처리 적용\n",
        "cleaned_texts = [preprocess_korean_text(text) for text in texts]\n",
        "print(\"전처리 완료!\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "bmaX7g3zDk8_",
        "outputId": "60747210-ab44-46e3-bbfd-8461ca5afc52"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'국내 최초 UV C LED 칫솔 살균기 개발 년 설립된 비에네스소프트는 소프트웨어 개발 기업이자 울트라웨이브 라는 브랜드로살균에 초점을 맞춘 퍼스널 헬스케어 제품을 개발 MIDDOT 제조 MIDDOT 수출하는 기업이다 독립채산제로 운영되는 특성상 회사 내 아이담 사업부에서 울트라웨이브 사업을 총괄한다 원래 아이담 사업부에서는 보이스레코더와 MP 같은 음향기기를 대부분 ODM 방식으로납품해 왔다 스마트폰 사용이 보편화되면서 음향기기 시장 규모가 줄어들자 친환경 반도체 광원인UV LED로 눈을 돌려 년 국내 최초로 UV C LED 칫솔 살균기 개발에 성공했다 기존 UV램프 칫솔 살균기는 UV램프 안에 수은이 들어 있고 살균 과정에서 오존을 발생기키기때문에 비릿한 냄새가 나는 단점이 있었다 UV LED 파장 중 UV C를 사용한 울트라웨이브칫솔 살균기는 분이면 뮤탄스균 MIDDOT 녹농균 MIDDOT 대장균 MIDDOT 황색포도상구균을 를 살균한다 수은을 함유하고 있지 않아 친환경적이고 오존을 발생시키지 않아 불쾌감 없이 쓸 수 있다 EU의 전기 MIDDOT 전자제품 유해물질사용제한 ROHS 인증을 취득했고 미국 FDA 승인 미국 FOC 인증 유럽 CD 인증도 완료했다 화상상담으로 해외 시장 확대 가능성을 보다비에네스소프트는 칫솔 살균기 면도기 살균기 마스크 살균기 등 UV LED 살균기 라인과 함께EARPET 반려동물의 귀 질환 MIDDOT 염증을 치료하는 동물의료기기 PEPPI 반려동물 전용 MP 등펫헬스케어 라인을 구성했지만 년 코로나 라는 변수를 맞았다 코로나 로 참가 예정이었던 해외 전시회가 줄줄이 취소되는 상황이 발생한 것이다 년에 해외 시장조사와 신규 바이어 영업을 위해 코트라에 도움을 요청했다면 년에는 영국 스페인 독일 미국 싱가포르 홍콩 말레이시아 태국 칠레 등 여러 국가의 기업들과 화상상담회를 진행했습니다 해외 시장을 확대하려는 중소기업 특히 신제품을 개발한 기업에게는 해외 전시회 참여가 필수인데 코로나 로 인해 해외 출장마저 갈 수 없는 막막한 상황에서 화상상담회는 희망이 되었습니다 특히 년 월 진행한 함부르크 무역관 화상상담회는 무엇보다 그 성과가 두드러진화상상담회였다 비에네스소프트는 월 초 함부르크 무역관으로부터 제품을 유통하고자 하는S사와의 화상상담을 제안 받아 월 일 화상상담을 진행했다 S사는 쿠쿠전자 유럽총판 지원업체로 아마존 독일 OTTO 등 유럽 내 온라인 플랫폼과 한국 롯데 하이마트와유사한 SATURN MEDIAMARK 등 오프라인 유통라인을 보유하고 있다 화상상담회 덕분에 독일의 S사에 샘플 물량을 추출할 수 있었다 세계 퍼스널 헬스케어 살균기 시장 위를 꿈꾸다 S사에서 울트라웨이브 제품에 대해 큰 관심과 기대감을 표현해 주셨습니다 제품 기능 위주로 질문과 답변이 이뤄졌고 양사가 추구하는 유통 방향에 대해서도 협의할 수 있었습니다 S사에 테스트 물량을 수출한 비에네스소프트는 현재 아마존 독일 론칭을 준비 중이다 또한 S사와 독일뿐 아니라 EU 전체로도 제품 유통이 가능한 지에 대해서도 논의하고 있다 비에네스소프트는 여러 차례의 화상상담회를 계기로 년 하반기부터 년까지 만 달러 규모의 신규 수출을 예상하고 있다 코로나 는 오히려 비에네스소프트에 새로운 기회를 가져다주었다 코로나 이전에는바이어에게 퍼스널 헬스케어 살균기가 왜 필요한지 오랜 시간을 들여 설득해야 했지만 코로나 대유행 이후부터는 그럴 필요가 없어졌다 년 세계 최초로 개발해 년국내 특허 등록까지 마친 마스크 살균기에 대한 관심도 폭발적으로 증가했다 살균기는 기본적으로 신뢰성을 담보해야 한다 비에네스소프트는 국내 MIDDOT 해외 인증 결과로고객에게 제품에 대한 신뢰를 제공하고 있다 여기에 한국의 성공적인 코로나 방역을전 세계가 주목하면서 한국 제품으로서의 가치가 더 상승했다 게다가 코트라와 함께해외 업체들과 접촉하기 때문에 비에네스소프트에 대한 신뢰를 더욱 높일 수 있었다 비에네스소프트 입장에서는 해외 시장에 대한 폭넓고 정확한 정보를 가진코트라를 통해 연결되는 기업이라면 신뢰할 수 있다는 믿음도 생겼다 사람과 반려동물을 대상으로 광 테라피를 이용한 헬스케어 제품을 보유한 비에네스소프트는앞으로 공기 살균기처럼 공간을 바꾸고 생활문화까지 바꿀 수 있는 제품을 선보일 계획인다 또한 현재 매출의 가 해외 시장에서 발생하고 있어 미국 남미를 중심으로해외 시장 개척 MIDDOT 확대에 더욱 박차를 가할 예정이다 코트라와 지속적인 협력을 통해 미주 유럽 아세안 서아시아 등으로 시장을 확대해 한국 브랜드인 울트라웨이브가 글로벌 퍼스널 헬스케어 살균기 시장 위로 우뚝 설 수 있도록 노력하겠습니다 출처 경제외교 해외진출 길을 넓히다 경제외교 기업활용 성과사례집'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "cleaned_texts[11]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvs1sKlBESeh"
      },
      "source": [
        "#### ============================================\n",
        "#### 3. 한국어 형태소 분석 및 불용어 처리\n",
        "#### ============================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-YfECdREOlv",
        "outputId": "9b2088cc-3643-4732-c437-b10186911199"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "3. 형태소 분석기 설정...\n",
            "============================================================\n",
            "형태소 분석기 설정 완료!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# 3. 한국어 형태소 분석 및 불용어 처리\n",
        "# ============================================\n",
        "print(\"=\" * 60)\n",
        "print(\"3. 형태소 분석기 설정...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Okt 형태소 분석기 초기화\n",
        "okt = Okt()\n",
        "# kkma = Kkma()\n",
        "# komoran = Komoran()\n",
        "# hannanum = Hannanum()\n",
        "\n",
        "# 한국어 불용어 리스트\n",
        "korean_stopwords = [\n",
        "    '의', '가', '이', '은', '들', '는', '좀', '잘', '걍', '과', '도', '를', '으로', '자',\n",
        "    '에', '와', '한', '하다', '을', '를', '에서', '으로', '로', '에게', '의해', '까지',\n",
        "    '수', '것', '등', '및', '그', '저', '것', '또', '또한', '더', '매우', '정말'\n",
        "]\n",
        "\n",
        "\n",
        "def tokenize_korean(text):\n",
        "    \"\"\"한국어 토큰화 (명사 추출)\"\"\"\n",
        "    # 명사만 추출\n",
        "    nouns = okt.nouns(text)\n",
        "    # nouns = kkma.nouns(text)\n",
        "    # nouns = komoran.nouns(text)\n",
        "    # nouns = hannanum.nouns(text)\n",
        "\n",
        "    # 불용어 제거 및 2글자 이상만 선택\n",
        "    tokens = [noun for noun in nouns if noun not in korean_stopwords and len(noun) > 1]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "print(\"형태소 분석기 설정 완료!\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "JB9HJc47mNiT",
        "outputId": "1de31e5e-2b61-4427-9824-eef620ba06be"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'국내 최초 칫솔 살균 개발 설립 에네스 소프트 소프트웨어 개발 기업 이자 울트라 웨이브 브랜드 살균 초점 퍼스 스케 제품 개발 제조 수출 기업 독립채산제 운영 특성 회사 아이 사업 울트라 웨이브 사업 총괄 원래 아이 사업 보이스 레코 음향 기기 대부분 방식 납품 스마트폰 사용 보편화 음향 기기 시장 규모 친환경 반도체 광원 국내 최초 칫솔 살균 개발 기존 램프 칫솔 살균 램프 수은 살균 과정 오존 발생 기키 때문 비릿 냄새 단점 파장 사용 울트라 웨이브 칫솔 살균 뮤탄스균 녹농균 대장균 황색포도상구균 살균 수은 함유 친환경 오존 발생 불쾌감 전기 전자제품 유해 물질 사용제한 인증 취득 미국 승인 미국 인증 유럽 인증 완료 화상 상담 해외 시장 확대 가능성 다비 에네스 소프트 칫솔 살균 면도기 살균 마스크 살균 살균 라인 반려동물 질환 염증 치료 동물 의료기기 반려동물 전용 펫헬 스케 라인 구성 코로나 변수 코로나 참가 예정 해외 전시회 줄줄이 취소 상황 발생 해외 시장조사 신규 바이어 영업 위해 코트라 도움 요청 영국 스페인 독일 미국 싱가포르 홍콩 말레이시아 태국 칠레 여러 국가 기업 화상 상담 진행 해외 시장 확대 중소기업 신제품 개발 기업 해외 전시회 참여 필수 코로나 해외 출장 상황 화상 상담 희망이 진행 함부르크 무역 화상 상담 무엇 성과 화상 상담 에네스 소프트 함부르크 무역 제품 유통 화상 상담 제안 화상 상담 진행 쿠쿠 전자 유럽 업체 아마존 독일 유럽 온라인 플랫폼 한국 롯데 하이마트 오프라인 유통 라인 보유 화상 상담 덕분 독일 샘플 물량 추출 세계 퍼스 스케 살균 시장 울트라 웨이브 제품 대해 관심 대감 표현 제품 기능 위주 질문 답변 양사 추구 유통 방향 대해 서도 협의 테스트 물량 수출 에네스 소프트 현재 아마존 독일 론칭 준비 중이 독일 전체 제품 유통 대해 서도 논의 에네스 소프트 여러 차례 화상 상담 계기 하반기 달러 규모 신규 수출 예상 코로나 오히려 에네스 소프트 기회 코로나 이전 바이어 퍼스 스케 살균 시간 설득 코로나 대유행 이후 필요 세계 최초 개발 국내 특허 등록 마스크 살균 대한 관심 폭발 증가 살균 기본 신뢰 담보 에네스 소프트 국내 해외 인증 결과 고객 제품 대한 신뢰 제공 여기 한국 성공 코로나 방역 세계 주목 한국 제품 가치 상승 게다가 코트라 해외 업체 접촉 때문 에네스 소프트 대한 신뢰 더욱 에네스 소프트 입장 해외 시장 대한 정보 코트라 통해 연결 기업 라면 신뢰 믿음 사람과 반려동물 대상 테라 이용 스케 제품 보유 에네스 소프트 공기 살균 처럼 공간 생활 문화 제품 계획 현재 매출 해외 시장 발생 미국 남미 중심 해외 시장 개척 확대 더욱 박차 예정 코트라 지속 협력 통해 미주 유럽 아세안 서아시아 시장 확대 한국 브랜드 울트라 웨이브 글로벌 퍼스 스케 살균 시장 위로 우뚝 노력 출처 경제 외교 해외진출 경제 외교 기업 활용 성과 사례'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "tokenize_korean(cleaned_texts[11])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0d9lcElG55g"
      },
      "source": [
        "#### ============================================\n",
        "#### 4. BERTopic 모델 설정\n",
        "#### ============================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9
          ]
        },
        "id": "DamFc-XtFMUq",
        "outputId": "9b9141d4-d7bc-4a00-832a-0ae11f7442e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "4. BERTopic 모델 설정 중...\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "738098161a29429da54e0496559d1af6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b50e56d3165468a91927d872e404cc7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "866a82ada61c4b9197bff9ba80b5b49d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bfb7ff6f24504a9b95059fded45be07a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "241483f1736944fbb6f1c2fd03999620"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f1b6a6700a3f4de18af1696384acb583"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da3a4f8f314d46b69b1f705ffc976e05"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e2e498fb026f433c989f1ce4ef74ce83"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6584227d21de44d3947ec1343784661c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ca97edf3b22946a899abc5331f373300"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERTopic 모델 설정 완료!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# 4. BERTopic 모델 설정\n",
        "# ============================================\n",
        "print(\"=\" * 60)\n",
        "print(\"4. BERTopic 모델 설정 중...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 4-1. 한국어 임베딩 모델 선택\n",
        "# 옵션 1: 다국어 모델 (추천)\n",
        "embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "\n",
        "# 옵션 2: 한국어 특화 모델 (더 좋은 성능을 원한다면)\n",
        "#embedding_model = SentenceTransformer('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')\n",
        "\n",
        "# 임베딩 생성 (progress bar 표시)\n",
        "# embeddings = embedding_model.encode(\n",
        "#     cleaned_texts,\n",
        "#     show_progress_bar=True,\n",
        "#     batch_size=32  # 배치 크기 (메모리에 따라 조정)\n",
        "# )\n",
        "# print(f\"임베딩 모델: {embedding_model}\")\n",
        "\n",
        "# 4-2. Vectorizer 설정 (형태소 분석 적용)\n",
        "vectorizer_model = CountVectorizer(\n",
        "    tokenizer=lambda x: x.split(),  # 이미 토큰화된 텍스트 사용\n",
        "    min_df=2,  # 최소 2번 이상 출현한 단어만 사용\n",
        "    max_df=0.95,  # 95% 이상의 문서에 나타난 단어 제외\n",
        "     ngram_range=(1, 2)  # 1-gram과 2-gram 모두 사용\n",
        ")\n",
        "\n",
        "# add clustering\n",
        "#cluster_model = KMeans(n_clusters=10)\n",
        "#cluster_model = AgglomerativeClustering(n_clusters=10)\n",
        "\n",
        "umap_model = UMAP(n_neighbors=7, n_components=5, min_dist=0.0, metric='cosine')\n",
        "\n",
        "hdbscan_model = HDBSCAN(min_cluster_size=10, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
        "\n",
        "\n",
        "# 4-3. BERTopic 모델 초기화\n",
        "topic_model = BERTopic(\n",
        "    embedding_model=embedding_model,\n",
        "    vectorizer_model=vectorizer_model,\n",
        "\n",
        "    umap_model=umap_model,  ## 추가\n",
        "    hdbscan_model=hdbscan_model, ## 추가\n",
        "    #hdbscan_model=cluster_model, ## 추가\n",
        "\n",
        "    language='korean',\n",
        "    calculate_probabilities=True,\n",
        "    verbose=True,\n",
        "    # min_topic_size=5,  # 최소 토픽 크기 (데이터 크기에 따라 조정)\n",
        "    # nr_topics='auto'  # 토픽 수 자동 결정\n",
        "    nr_topics=6  # 토픽 수 지정\n",
        "\n",
        ")\n",
        "\n",
        "print(\"BERTopic 모델 설정 완료!\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8aI3xtbHFXO"
      },
      "source": [
        "#### ============================================\n",
        "#### 5. 토픽 모델링 실행\n",
        "#### ============================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "IDIh-kyRG0o1",
        "outputId": "c6c1a418-a17c-4e9e-8855-2dbeed628a6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "5. 토픽 모델링 실행 중... (시간이 걸릴 수 있습니다)\n",
            "============================================================\n",
            "형태소 분석 진행 중...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/7 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ac180b035a44f3ba376673a6aebb2c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-10-16 23:08:35,803 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
            "2025-10-16 23:09:02,853 - BERTopic - Dimensionality - Completed ✓\n",
            "2025-10-16 23:09:02,854 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
            "2025-10-16 23:09:02,888 - BERTopic - Cluster - Completed ✓\n",
            "2025-10-16 23:09:02,890 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
            "2025-10-16 23:09:03,376 - BERTopic - Representation - Completed ✓\n",
            "2025-10-16 23:09:03,377 - BERTopic - Topic reduction - Reducing number of topics\n",
            "2025-10-16 23:09:03,379 - BERTopic - Topic reduction - Number of topics (6) is equal or higher than the clustered topics(3).\n",
            "2025-10-16 23:09:03,379 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
            "2025-10-16 23:09:04,089 - BERTopic - Representation - Completed ✓\n",
            "6it [00:00, 11.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "토픽 모델링 완료!\n",
            "발견된 토픽 수: 2개 (노이즈 제외)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# 5. 토픽 모델링 실행\n",
        "# ============================================\n",
        "print(\"=\" * 60)\n",
        "print(\"5. 토픽 모델링 실행 중... (시간이 걸릴 수 있습니다)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 형태소 분석 적용\n",
        "print(\"형태소 분석 진행 중...\")\n",
        "tokenized_texts = [tokenize_korean(text) for text in cleaned_texts]\n",
        "\n",
        "embeddings = embedding_model.encode(\n",
        "    tokenized_texts,\n",
        "    show_progress_bar=True,\n",
        "    batch_size=32  # 배치 크기 (메모리에 따라 조정)\n",
        ")\n",
        "\n",
        "# 토픽 모델 학습\n",
        "topics, probs = topic_model.fit_transform(tokenized_texts, embeddings)\n",
        "#topics, probs = topic_model.fit_transform(tokenized_texts)\n",
        "\n",
        "# Time Series by Topic\n",
        "topics_over_time = topic_model.topics_over_time(tokenized_texts, timestamps)\n",
        "\n",
        "\n",
        "print(f\"\\n토픽 모델링 완료!\")\n",
        "print(f\"발견된 토픽 수: {len(set(topics)) - 1}개 (노이즈 제외)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "ot6QefwQcb_0",
        "outputId": "57a9bcda-3d97-4744-9cd2-3db4c3ae1562"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'국내 최초 칫솔 살균 개발 설립 에네스 소프트 소프트웨어 개발 기업 이자 울트라 웨이브 브랜드 살균 초점 퍼스 스케 제품 개발 제조 수출 기업 독립채산제 운영 특성 회사 아이 사업 울트라 웨이브 사업 총괄 원래 아이 사업 보이스 레코 음향 기기 대부분 방식 납품 스마트폰 사용 보편화 음향 기기 시장 규모 친환경 반도체 광원 국내 최초 칫솔 살균 개발 기존 램프 칫솔 살균 램프 수은 살균 과정 오존 발생 기키 때문 비릿 냄새 단점 파장 사용 울트라 웨이브 칫솔 살균 뮤탄스균 녹농균 대장균 황색포도상구균 살균 수은 함유 친환경 오존 발생 불쾌감 전기 전자제품 유해 물질 사용제한 인증 취득 미국 승인 미국 인증 유럽 인증 완료 화상 상담 해외 시장 확대 가능성 다비 에네스 소프트 칫솔 살균 면도기 살균 마스크 살균 살균 라인 반려동물 질환 염증 치료 동물 의료기기 반려동물 전용 펫헬 스케 라인 구성 코로나 변수 코로나 참가 예정 해외 전시회 줄줄이 취소 상황 발생 해외 시장조사 신규 바이어 영업 위해 코트라 도움 요청 영국 스페인 독일 미국 싱가포르 홍콩 말레이시아 태국 칠레 여러 국가 기업 화상 상담 진행 해외 시장 확대 중소기업 신제품 개발 기업 해외 전시회 참여 필수 코로나 해외 출장 상황 화상 상담 희망이 진행 함부르크 무역 화상 상담 무엇 성과 화상 상담 에네스 소프트 함부르크 무역 제품 유통 화상 상담 제안 화상 상담 진행 쿠쿠 전자 유럽 업체 아마존 독일 유럽 온라인 플랫폼 한국 롯데 하이마트 오프라인 유통 라인 보유 화상 상담 덕분 독일 샘플 물량 추출 세계 퍼스 스케 살균 시장 울트라 웨이브 제품 대해 관심 대감 표현 제품 기능 위주 질문 답변 양사 추구 유통 방향 대해 서도 협의 테스트 물량 수출 에네스 소프트 현재 아마존 독일 론칭 준비 중이 독일 전체 제품 유통 대해 서도 논의 에네스 소프트 여러 차례 화상 상담 계기 하반기 달러 규모 신규 수출 예상 코로나 오히려 에네스 소프트 기회 코로나 이전 바이어 퍼스 스케 살균 시간 설득 코로나 대유행 이후 필요 세계 최초 개발 국내 특허 등록 마스크 살균 대한 관심 폭발 증가 살균 기본 신뢰 담보 에네스 소프트 국내 해외 인증 결과 고객 제품 대한 신뢰 제공 여기 한국 성공 코로나 방역 세계 주목 한국 제품 가치 상승 게다가 코트라 해외 업체 접촉 때문 에네스 소프트 대한 신뢰 더욱 에네스 소프트 입장 해외 시장 대한 정보 코트라 통해 연결 기업 라면 신뢰 믿음 사람과 반려동물 대상 테라 이용 스케 제품 보유 에네스 소프트 공기 살균 처럼 공간 생활 문화 제품 계획 현재 매출 해외 시장 발생 미국 남미 중심 해외 시장 개척 확대 더욱 박차 예정 코트라 지속 협력 통해 미주 유럽 아세안 서아시아 시장 확대 한국 브랜드 울트라 웨이브 글로벌 퍼스 스케 살균 시장 위로 우뚝 노력 출처 경제 외교 해외진출 경제 외교 기업 활용 성과 사례'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "tokenized_texts[11]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "Mwiso5n3VEUZ",
        "outputId": "4ad22f75-a821-4963-e378-fca185e6c7a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('베스트', 0.04221084972736996), ('쿠션', 0.04068948329128056), ('라이프', 0.03255158663302445), ('더블', 0.03212382444316177), ('바디', 0.031366131592670474), ('화장품 브랜드', 0.029346013075736536), ('에이', 0.029145782340035026), ('파트너 이후', 0.027445365143586663), ('리프', 0.02608075303584546), ('언니', 0.025588165402463493)]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Topic  Count               Name  \\\n",
              "0     -1     38  -1_장갑_부착_엔지니어링_국수   \n",
              "1      0    139      0_부품_건설_에스_장비   \n",
              "2      1     17    1_베스트_쿠션_라이프_더블   \n",
              "\n",
              "                                      Representation  \\\n",
              "0      [장갑, 부착, 엔지니어링, 국수, 재활, 콜롬비아, 사회, 불꽃, 구리, 할랄]   \n",
              "1        [부품, 건설, 에스, 장비, 공장, 식품, 설비, 납품, 화상 상담, 화상]   \n",
              "2  [베스트, 쿠션, 라이프, 더블, 바디, 화장품 브랜드, 에이, 파트너 이후, 리프...   \n",
              "\n",
              "                                 Representative_Docs  \n",
              "0  [장갑 아이 어데 오냐 간다 장갑 어데 가노 장갑 이름 해수 이해수 카노 장갑 맹그...  \n",
              "1  [기술 수출 엠씨 기업 소개 엠씨 기업인 홍기 설립 동안 산업 기계 중대 기계 가공...  \n",
              "2  [바디 제품 기업 세계 도전 라이프 투게더 기업 소개 송운 대표 라이프 투게더 설립...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-40af4c19-9b8a-47bb-868a-ef9e102805d2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Topic</th>\n",
              "      <th>Count</th>\n",
              "      <th>Name</th>\n",
              "      <th>Representation</th>\n",
              "      <th>Representative_Docs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1</td>\n",
              "      <td>38</td>\n",
              "      <td>-1_장갑_부착_엔지니어링_국수</td>\n",
              "      <td>[장갑, 부착, 엔지니어링, 국수, 재활, 콜롬비아, 사회, 불꽃, 구리, 할랄]</td>\n",
              "      <td>[장갑 아이 어데 오냐 간다 장갑 어데 가노 장갑 이름 해수 이해수 카노 장갑 맹그...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>139</td>\n",
              "      <td>0_부품_건설_에스_장비</td>\n",
              "      <td>[부품, 건설, 에스, 장비, 공장, 식품, 설비, 납품, 화상 상담, 화상]</td>\n",
              "      <td>[기술 수출 엠씨 기업 소개 엠씨 기업인 홍기 설립 동안 산업 기계 중대 기계 가공...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>1_베스트_쿠션_라이프_더블</td>\n",
              "      <td>[베스트, 쿠션, 라이프, 더블, 바디, 화장품 브랜드, 에이, 파트너 이후, 리프...</td>\n",
              "      <td>[바디 제품 기업 세계 도전 라이프 투게더 기업 소개 송운 대표 라이프 투게더 설립...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-40af4c19-9b8a-47bb-868a-ef9e102805d2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-40af4c19-9b8a-47bb-868a-ef9e102805d2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-40af4c19-9b8a-47bb-868a-ef9e102805d2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-6a4a008a-ac53-4edd-b234-65830afed458\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6a4a008a-ac53-4edd-b234-65830afed458')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-6a4a008a-ac53-4edd-b234-65830afed458 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"topic_model\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Topic\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": -1,\n        \"max\": 1,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -1,\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 65,\n        \"min\": 17,\n        \"max\": 139,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          38,\n          139,\n          17\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"-1_\\uc7a5\\uac11_\\ubd80\\ucc29_\\uc5d4\\uc9c0\\ub2c8\\uc5b4\\ub9c1_\\uad6d\\uc218\",\n          \"0_\\ubd80\\ud488_\\uac74\\uc124_\\uc5d0\\uc2a4_\\uc7a5\\ube44\",\n          \"1_\\ubca0\\uc2a4\\ud2b8_\\ucfe0\\uc158_\\ub77c\\uc774\\ud504_\\ub354\\ube14\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Representation\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Representative_Docs\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "print(topic_model.get_topic(1))\n",
        "topic_model.get_topic_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QgB1u8rHWtcX"
      },
      "outputs": [],
      "source": [
        "#topic_model.get_representative_docs(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-OxP1mMLwuW"
      },
      "source": [
        "#### ============================================\n",
        "#### 6. 결과 분석\n",
        "#### ============================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8b1joKCLrTf",
        "outputId": "5850a7bb-af45-4225-ef48-bf3e25c59617"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "6. 토픽 분석 결과\n",
            "============================================================\n",
            "\n",
            "[토픽별 문서 수]\n",
            "토픽 -1 (노이즈): 38개\n",
            "토픽 0: 139개\n",
            "토픽 1: 17개\n",
            "\n",
            "[각 토픽의 주요 키워드 (상위 10개)]\n",
            "\n",
            "토픽 0:\n",
            "  - 부품: 0.0286\n",
            "  - 건설: 0.0272\n",
            "  - 에스: 0.0262\n",
            "  - 장비: 0.0254\n",
            "  - 공장: 0.0252\n",
            "  - 식품: 0.0250\n",
            "  - 설비: 0.0216\n",
            "  - 납품: 0.0208\n",
            "  - 화상 상담: 0.0201\n",
            "  - 화상: 0.0200\n",
            "\n",
            "토픽 1:\n",
            "  - 베스트: 0.0422\n",
            "  - 쿠션: 0.0407\n",
            "  - 라이프: 0.0326\n",
            "  - 더블: 0.0321\n",
            "  - 바디: 0.0314\n",
            "  - 화장품 브랜드: 0.0293\n",
            "  - 에이: 0.0291\n",
            "  - 파트너 이후: 0.0274\n",
            "  - 리프: 0.0261\n",
            "  - 언니: 0.0256\n",
            "\n",
            "[토픽 정보 요약]\n",
            "   Topic  Count               Name  \\\n",
            "0     -1     38  -1_장갑_부착_엔지니어링_국수   \n",
            "1      0    139      0_부품_건설_에스_장비   \n",
            "2      1     17    1_베스트_쿠션_라이프_더블   \n",
            "\n",
            "                                      Representation  \\\n",
            "0      [장갑, 부착, 엔지니어링, 국수, 재활, 콜롬비아, 사회, 불꽃, 구리, 할랄]   \n",
            "1        [부품, 건설, 에스, 장비, 공장, 식품, 설비, 납품, 화상 상담, 화상]   \n",
            "2  [베스트, 쿠션, 라이프, 더블, 바디, 화장품 브랜드, 에이, 파트너 이후, 리프...   \n",
            "\n",
            "                                 Representative_Docs  \n",
            "0  [장갑 아이 어데 오냐 간다 장갑 어데 가노 장갑 이름 해수 이해수 카노 장갑 맹그...  \n",
            "1  [기술 수출 엠씨 기업 소개 엠씨 기업인 홍기 설립 동안 산업 기계 중대 기계 가공...  \n",
            "2  [바디 제품 기업 세계 도전 라이프 투게더 기업 소개 송운 대표 라이프 투게더 설립...  \n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# 6. 결과 분석\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"6. 토픽 분석 결과\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 토픽별 문서 수\n",
        "topic_counts = pd.Series(topics).value_counts().sort_index()\n",
        "print(\"\\n[토픽별 문서 수]\")\n",
        "for topic_id, count in topic_counts.items():\n",
        "    if topic_id == -1:\n",
        "        print(f\"토픽 {topic_id} (노이즈): {count}개\")\n",
        "    else:\n",
        "        print(f\"토픽 {topic_id}: {count}개\")\n",
        "\n",
        "# 각 토픽의 상위 키워드\n",
        "print(\"\\n[각 토픽의 주요 키워드 (상위 10개)]\")\n",
        "for topic_id in sorted(set(topics)):\n",
        "    if topic_id != -1:  # 노이즈 제외\n",
        "        keywords = topic_model.get_topic(topic_id)\n",
        "        print(f\"\\n토픽 {topic_id}:\")\n",
        "        for word, score in keywords[:10]:\n",
        "            print(f\"  - {word}: {score:.4f}\")\n",
        "\n",
        "# 토픽 정보 DataFrame\n",
        "topic_info = topic_model.get_topic_info()\n",
        "print(\"\\n[토픽 정보 요약]\")\n",
        "print(topic_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_9W8OgmHC07"
      },
      "source": [
        "#### ============================================\n",
        "#### 7. 시각화 1: 원본 임베딩 사용\n",
        "#### ============================================\n",
        "#### ============================================\n",
        "#### 8. 시각화 2: UMAP 차원 축소 후 시각화\n",
        "#### ============================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTTBlUOfGHK7",
        "outputId": "462a4499-8cb7-4108-bf46-17aefb47f5fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "7단계: 문서 시각화 (원본 고차원 임베딩)\n",
            "======================================================================\n",
            "✓ 시각화 1 완료: 'bertopic_visualization_original.html' 저장\n",
            "  → 원본 고차원 임베딩을 사용한 문서 분포\n",
            "\n",
            "======================================================================\n",
            "8단계: UMAP 차원 축소 후 시각화 (더 빠른 반복 분석)\n",
            "======================================================================\n",
            "✓ UMAP 차원 축소 진행 중...\n",
            "✓ 차원 축소 완료: (194, 384) → (194, 2)\n",
            "✓ 시각화 2 완료: 'bertopic_visualization_umap.html' 저장\n",
            "  → UMAP 차원 축소를 적용한 문서 분포 (빠른 렌더링)\n",
            "\n",
            "======================================================================\n",
            "9단계: 추가 시각화\n",
            "======================================================================\n",
            "⚠ 일부 추가 시각화 생성 중 오류: zero-size array to reduction operation maximum which has no identity\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# 7. 시각화 1: 원본 임베딩 사용\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"7단계: 문서 시각화 (원본 고차원 임베딩)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "try:\n",
        "    # 원본 임베딩으로 시각화 (내부적으로 UMAP 적용)\n",
        "    fig1 = topic_model.visualize_documents(\n",
        "        cleaned_texts,\n",
        "        embeddings=embeddings,\n",
        "        hide_annotations=False,\n",
        "        hide_document_hover=False,\n",
        "        custom_labels=True\n",
        "    )\n",
        "\n",
        "    # HTML 파일로 저장\n",
        "    fig1.write_html('bertopic_visualization_original.html')\n",
        "    print(\"✓ 시각화 1 완료: 'bertopic_visualization_original.html' 저장\")\n",
        "    print(\"  → 원본 고차원 임베딩을 사용한 문서 분포\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"⚠ 시각화 1 생성 중 오류: {e}\")\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# 8. 시각화 2: UMAP 차원 축소 후 시각화\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"8단계: UMAP 차원 축소 후 시각화 (더 빠른 반복 분석)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "try:\n",
        "    # UMAP으로 차원 축소 (고차원 → 2차원)\n",
        "    print(\"✓ UMAP 차원 축소 진행 중...\")\n",
        "    reduced_embeddings = UMAP(\n",
        "        n_neighbors=7,      # 이웃 수\n",
        "        n_components=2,      # 2차원으로 축소\n",
        "        min_dist=0.0,        # 최소 거리\n",
        "        metric='cosine',     # 코사인 유사도\n",
        "        random_state=42      # 재현성\n",
        "    ).fit_transform(embeddings)\n",
        "\n",
        "    print(f\"✓ 차원 축소 완료: {embeddings.shape} → {reduced_embeddings.shape}\")\n",
        "\n",
        "    # 축소된 임베딩으로 시각화 (빠른 속도)\n",
        "    fig2 = topic_model.visualize_documents(\n",
        "        tokenized_texts,\n",
        "        reduced_embeddings=reduced_embeddings,\n",
        "        hide_annotations=False,\n",
        "        hide_document_hover=False,\n",
        "        custom_labels=True\n",
        "    )\n",
        "\n",
        "    # HTML 파일로 저장\n",
        "    fig2.write_html('bertopic_visualization_umap.html')\n",
        "    print(\"✓ 시각화 2 완료: 'bertopic_visualization_umap.html' 저장\")\n",
        "    print(\"  → UMAP 차원 축소를 적용한 문서 분포 (빠른 렌더링)\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"⚠ 시각화 2 생성 중 오류: {e}\")\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# 9. 추가 시각화\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"9단계: 추가 시각화\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "try:\n",
        "    # 토픽 간 거리 시각화\n",
        "    fig3 = topic_model.visualize_topics()\n",
        "    fig3.write_html('topic_distance_map.html')\n",
        "    print(\"✓ 토픽 거리 맵: 'topic_distance_map.html'\")\n",
        "\n",
        "    # 토픽 바차트\n",
        "    fig4 = topic_model.visualize_barchart(top_n_topics=15, n_words=10)\n",
        "    fig4.write_html('topic_barchart.html')\n",
        "    print(\"✓ 토픽 바차트: 'topic_barchart.html'\")\n",
        "\n",
        "    # 토픽 계층 구조\n",
        "    hierarchical_topics = topic_model.hierarchical_topics(tokenized_texts)\n",
        "    fig5 = topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)\n",
        "    fig5.write_html('topic_hierarchy.html')\n",
        "    print(\"✓ 토픽 계층 구조: 'topic_hierarchy.html'\")\n",
        "\n",
        "    # 토픽 히트맵\n",
        "    fig6 = topic_model.visualize_heatmap()\n",
        "    fig6.write_html('topic_heatmap.html')\n",
        "    print(\"✓ 토픽 유사도 히트맵: 'topic_heatmap.html'\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"⚠ 일부 추가 시각화 생성 중 오류: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lyQmUU2MVl-",
        "outputId": "926ed326-40b1-4402-aafc-8ae07f4c2010"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "10. 결과 저장 중...\n",
            "============================================================\n",
            "토픽 정보가 'topic_info.csv'에 저장되었습니다.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ============================================\n",
        "# 10. 결과 저장\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"10. 결과 저장 중...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 원본 데이터에 토픽 정보 추가\n",
        "df_with_topics = df[df['bdtCntnt'].notna()].copy()\n",
        "df_with_topics['topic'] = topics\n",
        "df_with_topics['topic_probability'] = [prob.max() for prob in probs]\n",
        "\n",
        "# 결과 저장\n",
        "# df_with_topics.to_csv('bertopic_results.csv', index=False, encoding='utf-8-sig')\n",
        "# print(\"결과가 'bertopic_results.csv'에 저장되었습니다.\")\n",
        "\n",
        "# 토픽 정보 저장\n",
        "topic_info.to_csv('topic_info.csv', index=False, encoding='utf-8-sig')\n",
        "print(\"토픽 정보가 'topic_info.csv'에 저장되었습니다.\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#topic_model.visualize_distribution(topic_model.probabilities_[11])"
      ],
      "metadata": {
        "id": "ObAMh0q9Y9iv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-5qoAiNXBT_"
      },
      "source": [
        "- Time Series by Topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_63GxtbNTSxf",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#topic_model.visualize_topics_over_time(topics_over_time)\n",
        "#topic_model.visualize_topics_over_time(topics_over_time, topics=[0, 1, 2, 3, 4, 5, 6, 7])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TXBG23MXK-x"
      },
      "source": [
        "- Hierarchical Topic Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-fJM4zrOXg2F"
      },
      "outputs": [],
      "source": [
        "#topic_model.visualize_hierarchy()\n",
        "#topic_model.visualize_heatmap()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMzvJgecXSPK"
      },
      "source": [
        "- Topic Word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "JVItKJdZainS"
      },
      "outputs": [],
      "source": [
        "#topic_model.visualize_barchart(top_n_topics=10, n_words=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tupnGTkvR10y"
      },
      "outputs": [],
      "source": [
        "# # ============================================\n",
        "# # 11. 추가 분석 함수\n",
        "# # ============================================\n",
        "\n",
        "# def find_similar_documents(topic_model, text, top_n=5):\n",
        "#     \"\"\"특정 텍스트와 유사한 문서 찾기\"\"\"\n",
        "#     tokenized = tokenize_korean(preprocess_korean_text(text))\n",
        "#     similar_topics, similarity = topic_model.find_topics(tokenized, top_n=top_n)\n",
        "#     return similar_topics, similarity\n",
        "\n",
        "# def get_representative_docs(topic_model, topic_id, n_docs=3):\n",
        "#     \"\"\"특정 토픽의 대표 문서 가져오기\"\"\n",
        "#     return topic_model.get_representative_docs(topic_id)[:n_docs]\n",
        "\n",
        "# # 사용 예시\n",
        "# print(\"\\n[추가 분석 예시]\")\n",
        "# print(\"특정 토픽의 대표 문서를 보려면:\")\n",
        "# print(\"docs = get_representative_docs(topic_model, topic_id=0, n_docs=3)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hq67snFfSFTB"
      },
      "outputs": [],
      "source": [
        "# docs = get_representative_docs(topic_model, topic_id=1, n_docs=3)\n",
        "# docs[1]"
      ]
    }
  ]