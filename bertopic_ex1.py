# -*- coding: utf-8 -*-
"""Bertopic_ex1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ltXORH6rDOZ_wJYqenm3ZAexCfD698UV
"""

# ==========================================
# 1. Package Installation
# ==========================================

!pip install bertopic
#!pip install --upgrade gensim

# Choose an embedding backend
!pip install bertopic[flair,gensim,spacy,use,visualization]

# Topic modeling with images
# !pip install bertopic[vision]

!pip install konlpy
!pip install pyLDAvis

"""
한국어 텍스트 데이터를 위한 BERTopic 모델링
파일: trade_success_case_data.csv의 bdtCntnt 컬럼 분석
"""

import pandas as pd
import numpy as np
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from konlpy.tag import Okt, Hannanum, Kkma, Komoran
#from konlpy.tag import Okt

from sklearn.cluster import KMeans, AgglomerativeClustering
from umap import UMAP
from hdbscan import HDBSCAN

import string
import re
import warnings
warnings.filterwarnings('ignore')

"""#### ============================================
#### 1. 데이터 로드
#### ============================================
"""

# ============================================
# 1. 데이터 로드
# ============================================
print("=" * 60)
print("1. 데이터 로드 중...")
print("=" * 60)

df = pd.read_csv('https://raw.githubusercontent.com/datadigger01/AI-Trade/main/Data/trade_success_case_data.csv')
df['othbcDt'] = pd.to_datetime(df['othbcDt'])

print(f"전체 데이터 수: {len(df)}")

# bdtCntnt 컬럼 추출 및 결측치 제거
texts = df['bdtCntnt'].dropna().tolist()
timestamps = df['othbcDt'].dt.year.to_list() # 토픽별 Time Series를 위한 년단위 timestamps

print(f"유효한 텍스트 수: {len(texts)}")
print(f"평균 텍스트 길이: {np.mean([len(text) for text in texts]):.0f}자\n")
print(f"데이터셋 정보:{df.info()}")

texts[11]
#timestamps[3]

"""#### ============================================
#### 2. 텍스트 전처리
#### ============================================
"""

# ============================================
# 2. 텍스트 전처리
# ============================================
print("=" * 60)
print("2. 텍스트 전처리 중...")
print("=" * 60)

def combine_specific_words(text, word_pairs):
    """특정 단어 조합을 하나로 합치는 함수"""
    for word_pair in word_pairs:
        # 공백이 있는 형태를 공백 없는 형태로 변환
        text = text.replace(word_pair, word_pair.replace(' ', ''))
    return text

def replace_specific_words(text, word_dict):
    """특정 단어를 다른 단어로 치환하는 함수"""
    for old_word, new_word in word_dict.items():
        text = text.replace(old_word, new_word)
    return text

def preprocess_korean_text(text):
    """한국어 텍스트 전처리"""
    if pd.isna(text):
        return ""  ## doucument가 결측인경우 NaN return

    #########  특수문자 포함 정제처리할 단어 처리(삭제)
    # HTML 태그 제거
    pattern = r'<[^>]*>'
    text = re.sub(pattern, '', text)
    ## HTML 태그 제거
    ##text = re.sub(r'&[a-z]+;', ' ', text)

    # E-mail제거
    pattern = r'([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+)'
    text = re.sub(pattern, '', text)

    # URL제거
    pattern = r'(http|ftp|https)://(?:[-\w.]|(?:%[\da-fA-F]{2}))+'
    text = re.sub(pattern, '', text)
    ## Remove URLs
    # text = re.sub(r'http\S+|www.\S+', '', text)

    # 특수문자 제거 (한글, 영문, 공백만 유지)
    text = re.sub(r'[^가-힣a-zA-Z\s]', ' ', text)
    #text = re.sub(r'[^가-힣a-zA-Z0-9\s]', ' ', text)

    # 한글 자음, 모음 제거
    pattern = r'([ㄱ-ㅎㅏ-ㅣ]+)'
    text = re.sub(pattern, '', text)

    # 연속된 공백 제거
    text = re.sub(r'\s+', ' ', text)
    # # 연속 공백 정리
    # text = ' '.join(text.split())

    # 특수기호제거
    #pattern = r'[^\w\s]'
    text = re.sub(r'[^\w\s]', '', text)

    # Remove mentions and hashtags
    text = re.sub(r'@\w+|#\w+', '', text)

    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))

    # 영어는 모두 대문자로 전환
    text = text.upper()

    ###########################################################################
    ##  특정 단어에 대한 변환, 조합 및 제거
    ###########################################################################
    # 토큰화 전에 특정 단어를 다른 단어로 치환
    word_replacements = {
        'KOTRA': '코트라',
        '어플리케이션': '애플리케이션',
        '카메라': '캠코더',  # 예시
        # 필요한 단어 치환을 여기에 추가하세요
    }
    text = replace_specific_words(text, word_replacements)

    # 토큰화 전에 특정 단어 조합을 먼저 결합
    word_pairs_to_combine = [
        '성공 사례',
        '중소 기업',
        '수출 전략',
        '시장 조사',
        '데이터 분석',
        '제품 개발',
        '생활 용품'
        # 필요한 단어 조합을 여기에 추가하세요
    ]
    text = combine_specific_words(text, word_pairs_to_combine)

    # 특정 단어 제거
    remove_words = ['바로가기',
                    '성공사례',
                    'RSQUO','LSQUO','LDQUO','RDQUO',
                    '원문 슬기로운 코트라 활용법',
                    '수출전문위원이 전하는 중소기업 수출성공스토리'
                    ]
    for word in remove_words:
        text = text.replace(word, '')

    return text.strip()

# 전처리 적용
cleaned_texts = [preprocess_korean_text(text) for text in texts]
print("전처리 완료!\n")

cleaned_texts[11]

"""#### ============================================
#### 3. 한국어 형태소 분석 및 불용어 처리
#### ============================================
"""

# ============================================
# 3. 한국어 형태소 분석 및 불용어 처리
# ============================================
print("=" * 60)
print("3. 형태소 분석기 설정...")
print("=" * 60)

# Okt 형태소 분석기 초기화
okt = Okt()
# kkma = Kkma()
# komoran = Komoran()
# hannanum = Hannanum()

# 한국어 불용어 리스트
korean_stopwords = [
    '의', '가', '이', '은', '들', '는', '좀', '잘', '걍', '과', '도', '를', '으로', '자',
    '에', '와', '한', '하다', '을', '를', '에서', '으로', '로', '에게', '의해', '까지',
    '수', '것', '등', '및', '그', '저', '것', '또', '또한', '더', '매우', '정말'
]


def tokenize_korean(text):
    """한국어 토큰화 (명사 추출)"""
    # 명사만 추출
    nouns = okt.nouns(text)
    # nouns = kkma.nouns(text)
    # nouns = komoran.nouns(text)
    # nouns = hannanum.nouns(text)

    # 불용어 제거 및 2글자 이상만 선택
    tokens = [noun for noun in nouns if noun not in korean_stopwords and len(noun) > 1]
    return ' '.join(tokens)

print("형태소 분석기 설정 완료!\n")

tokenize_korean(cleaned_texts[11])

"""#### ============================================
#### 4. BERTopic 모델 설정
#### ============================================
"""

# ============================================
# 4. BERTopic 모델 설정
# ============================================
print("=" * 60)
print("4. BERTopic 모델 설정 중...")
print("=" * 60)

# 4-1. 한국어 임베딩 모델 선택
# 옵션 1: 다국어 모델 (추천)
embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# 옵션 2: 한국어 특화 모델 (더 좋은 성능을 원한다면)
#embedding_model = SentenceTransformer('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')

# 임베딩 생성 (progress bar 표시)
# embeddings = embedding_model.encode(
#     cleaned_texts,
#     show_progress_bar=True,
#     batch_size=32  # 배치 크기 (메모리에 따라 조정)
# )
# print(f"임베딩 모델: {embedding_model}")

# 4-2. Vectorizer 설정 (형태소 분석 적용)
vectorizer_model = CountVectorizer(
    tokenizer=lambda x: x.split(),  # 이미 토큰화된 텍스트 사용
    min_df=2,  # 최소 2번 이상 출현한 단어만 사용
    max_df=0.80,  # 95% 이상의 문서에 나타난 단어 제외
     ngram_range=(1, 2)  # 1-gram과 2-gram 모두 사용
)

# add clustering
#cluster_model = KMeans(n_clusters=10)
#cluster_model = AgglomerativeClustering(n_clusters=10)

umap_model = UMAP(n_neighbors=7, n_components=5, min_dist=0.0, metric='cosine')

hdbscan_model = HDBSCAN(min_cluster_size=5, metric='euclidean', cluster_selection_method='eom', prediction_data=True)


# 4-3. BERTopic 모델 초기화
topic_model = BERTopic(
    embedding_model=embedding_model,
    vectorizer_model=vectorizer_model,

    umap_model=umap_model,  ## 추가
    hdbscan_model=hdbscan_model, ## 추가
    #hdbscan_model=cluster_model, ## 추가

    language='korean',
    calculate_probabilities=True,
    verbose=True,
    # min_topic_size=5,  # 최소 토픽 크기 (데이터 크기에 따라 조정)
    # nr_topics='auto'  # 토픽 수 자동 결정
    nr_topics=6  # 토픽 수 지정

)

print("BERTopic 모델 설정 완료!\n")

"""#### ============================================
#### 5. 토픽 모델링 실행
#### ============================================
"""

# ============================================
# 5. 토픽 모델링 실행
# ============================================
print("=" * 60)
print("5. 토픽 모델링 실행 중... (시간이 걸릴 수 있습니다)")
print("=" * 60)

# 형태소 분석 적용
print("형태소 분석 진행 중...")
tokenized_texts = [tokenize_korean(text) for text in cleaned_texts]

embeddings = embedding_model.encode(
    tokenized_texts,
    show_progress_bar=True,
    batch_size=32  # 배치 크기 (메모리에 따라 조정)
)

# 토픽 모델 학습
topics, probs = topic_model.fit_transform(tokenized_texts, embeddings)
#topics, probs = topic_model.fit_transform(tokenized_texts)

# Time Series by Topic
topics_over_time = topic_model.topics_over_time(tokenized_texts, timestamps)


print(f"\n토픽 모델링 완료!")
print(f"발견된 토픽 수: {len(set(topics)) - 1}개 (노이즈 제외)")

tokenized_texts[11]

print(topic_model.get_topic(1))
topic_model.get_topic_info()

#topic_model.get_representative_docs(4)

"""#### ============================================
#### 6. 결과 분석
#### ============================================
"""

# ============================================
# 6. 결과 분석
# ============================================
print("\n" + "=" * 60)
print("6. 토픽 분석 결과")
print("=" * 60)

# 토픽별 문서 수
topic_counts = pd.Series(topics).value_counts().sort_index()
print("\n[토픽별 문서 수]")
for topic_id, count in topic_counts.items():
    if topic_id == -1:
        print(f"토픽 {topic_id} (노이즈): {count}개")
    else:
        print(f"토픽 {topic_id}: {count}개")

# 각 토픽의 상위 키워드
print("\n[각 토픽의 주요 키워드 (상위 10개)]")
for topic_id in sorted(set(topics)):
    if topic_id != -1:  # 노이즈 제외
        keywords = topic_model.get_topic(topic_id)
        print(f"\n토픽 {topic_id}:")
        for word, score in keywords[:10]:
            print(f"  - {word}: {score:.4f}")

# 토픽 정보 DataFrame
topic_info = topic_model.get_topic_info()
print("\n[토픽 정보 요약]")
print(topic_info)

"""#### ============================================
#### 7. 시각화 1: 원본 임베딩 사용
#### ============================================
#### ============================================
#### 8. 시각화 2: UMAP 차원 축소 후 시각화
#### ============================================
"""

# ============================================
# 7. 시각화 1: 원본 임베딩 사용
# ============================================
print("\n" + "=" * 70)
print("7단계: 문서 시각화 (원본 고차원 임베딩)")
print("=" * 70)

try:
    # 원본 임베딩으로 시각화 (내부적으로 UMAP 적용)
    fig1 = topic_model.visualize_documents(
        cleaned_texts,
        embeddings=embeddings,
        hide_annotations=False,
        hide_document_hover=False,
        custom_labels=True
    )

    # HTML 파일로 저장
    fig1.write_html('bertopic_visualization_original.html')
    print("✓ 시각화 1 완료: 'bertopic_visualization_original.html' 저장")
    print("  → 원본 고차원 임베딩을 사용한 문서 분포")

except Exception as e:
    print(f"⚠ 시각화 1 생성 중 오류: {e}")


# ============================================
# 8. 시각화 2: UMAP 차원 축소 후 시각화
# ============================================
print("\n" + "=" * 70)
print("8단계: UMAP 차원 축소 후 시각화 (더 빠른 반복 분석)")
print("=" * 70)

try:
    # UMAP으로 차원 축소 (고차원 → 2차원)
    print("✓ UMAP 차원 축소 진행 중...")
    reduced_embeddings = UMAP(
        n_neighbors=7,      # 이웃 수
        n_components=2,      # 2차원으로 축소
        min_dist=0.0,        # 최소 거리
        metric='cosine',     # 코사인 유사도
        random_state=42      # 재현성
    ).fit_transform(embeddings)

    print(f"✓ 차원 축소 완료: {embeddings.shape} → {reduced_embeddings.shape}")

    # 축소된 임베딩으로 시각화 (빠른 속도)
    fig2 = topic_model.visualize_documents(
        tokenized_texts,
        reduced_embeddings=reduced_embeddings,
        hide_annotations=False,
        hide_document_hover=False,
        custom_labels=True
    )

    # HTML 파일로 저장
    fig2.write_html('bertopic_visualization_umap.html')
    print("✓ 시각화 2 완료: 'bertopic_visualization_umap.html' 저장")
    print("  → UMAP 차원 축소를 적용한 문서 분포 (빠른 렌더링)")

except Exception as e:
    print(f"⚠ 시각화 2 생성 중 오류: {e}")


# ============================================
# 9. 추가 시각화
# ============================================
print("\n" + "=" * 70)
print("9단계: 추가 시각화")
print("=" * 70)

try:
    # 토픽 간 거리 시각화
    fig3 = topic_model.visualize_topics()
    fig3.write_html('topic_distance_map.html')
    print("✓ 토픽 거리 맵: 'topic_distance_map.html'")

    # 토픽 바차트
    fig4 = topic_model.visualize_barchart(top_n_topics=15, n_words=10)
    fig4.write_html('topic_barchart.html')
    print("✓ 토픽 바차트: 'topic_barchart.html'")

    # 토픽 계층 구조
    hierarchical_topics = topic_model.hierarchical_topics(tokenized_texts)
    fig5 = topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)
    fig5.write_html('topic_hierarchy.html')
    print("✓ 토픽 계층 구조: 'topic_hierarchy.html'")

    # 토픽 히트맵
    fig6 = topic_model.visualize_heatmap()
    fig6.write_html('topic_heatmap.html')
    print("✓ 토픽 유사도 히트맵: 'topic_heatmap.html'")

except Exception as e:
    print(f"⚠ 일부 추가 시각화 생성 중 오류: {e}")

# ============================================
# 10. 결과 저장
# ============================================
print("\n" + "=" * 60)
print("10. 결과 저장 중...")
print("=" * 60)

# 원본 데이터에 토픽 정보 추가
df_with_topics = df[df['bdtCntnt'].notna()].copy()
df_with_topics['topic'] = topics
df_with_topics['topic_probability'] = [prob.max() for prob in probs]

# 결과 저장
# df_with_topics.to_csv('bertopic_results.csv', index=False, encoding='utf-8-sig')
# print("결과가 'bertopic_results.csv'에 저장되었습니다.")

# 토픽 정보 저장
topic_info.to_csv('topic_info.csv', index=False, encoding='utf-8-sig')
print("토픽 정보가 'topic_info.csv'에 저장되었습니다.\n")

#topic_model.visualize_distribution(topic_model.probabilities_[11])

"""- Time Series by Topic"""

#topic_model.visualize_topics_over_time(topics_over_time)
#topic_model.visualize_topics_over_time(topics_over_time, topics=[0, 1, 2, 3, 4, 5, 6, 7])

"""- Hierarchical Topic Clustering"""

#topic_model.visualize_hierarchy()
#topic_model.visualize_heatmap()

"""- Topic Word"""

#topic_model.visualize_barchart(top_n_topics=10, n_words=10)

# # ============================================
# # 11. 추가 분석 함수
# # ============================================

# def find_similar_documents(topic_model, text, top_n=5):
#     """특정 텍스트와 유사한 문서 찾기"""
#     tokenized = tokenize_korean(preprocess_korean_text(text))
#     similar_topics, similarity = topic_model.find_topics(tokenized, top_n=top_n)
#     return similar_topics, similarity

# def get_representative_docs(topic_model, topic_id, n_docs=3):
#     """특정 토픽의 대표 문서 가져오기""
#     return topic_model.get_representative_docs(topic_id)[:n_docs]

# # 사용 예시
# print("\n[추가 분석 예시]")
# print("특정 토픽의 대표 문서를 보려면:")
# print("docs = get_representative_docs(topic_model, topic_id=0, n_docs=3)")

# docs = get_representative_docs(topic_model, topic_id=1, n_docs=3)
# docs[1]